{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import easyocr\n",
    "# from IPython.display import display\n",
    "\n",
    "from sudulunu.helpers import pp, make_num, dumper\n",
    "\n",
    "### This is all adapted from https://huggingface.co/spaces/pierreguillou/tatr-demo/blob/main/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "\n",
    "device =  \"cpu\"\n",
    "\n",
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "    # def __init__(self, max_size=1500):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n",
    "        \n",
    "        return resized_image\n",
    "\n",
    "# load table detection model\n",
    "# processor = TableTransformerImageProcessor(max_size=800)\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\", revision=\"no_timm\").to(device)\n",
    "\n",
    "# load table structure recognition model\n",
    "# structure_processor = TableTransformerImageProcessor(max_size=1000)\n",
    "structure_model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-structure-recognition-v1.1-all\").to(device)\n",
    "\n",
    "# load EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "detection_transform = transforms.Compose([\n",
    "    MaxResize(800),\n",
    "    # MaxResize(1500),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "structure_transform = transforms.Compose([\n",
    "    MaxResize(1000),\n",
    "        # MaxResize(1500),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_crop_table(image):\n",
    "    # prepare image for the model\n",
    "    # pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = detection_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # postprocess to get detected tables\n",
    "    id2label = model.config.id2label\n",
    "    id2label[len(model.config.id2label)] = \"no object\"\n",
    "    detected_tables = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    # crop first detected table out of image\n",
    "    cropped_table = image.crop(detected_tables[0][\"bbox\"])\n",
    "\n",
    "    return cropped_table\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    width, height = size\n",
    "    boxes = box_cxcywh_to_xyxy(out_bbox)\n",
    "    boxes = boxes * torch.tensor([width, height, width, height], dtype=torch.float32)\n",
    "    return boxes\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == 'no object':\n",
    "            objects.append({'label': class_label, 'score': float(score),\n",
    "                            'bbox': [float(elem) for elem in bbox]})\n",
    "\n",
    "    return objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == 'no object':\n",
    "            objects.append({'label': class_label, 'score': float(score),\n",
    "                            'bbox': [float(elem) for elem in bbox]})\n",
    "\n",
    "    return objects\n",
    "\n",
    "def recognize_table(image):\n",
    "    # prepare image for the model\n",
    "    # pixel_values = structure_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = structure_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = structure_model(pixel_values)\n",
    "\n",
    "    # postprocess to get individual elements\n",
    "    id2label = structure_model.config.id2label\n",
    "    id2label[len(structure_model.config.id2label)] = \"no object\"\n",
    "    cells = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    # visualize cells on cropped table\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for cell in cells:\n",
    "        draw.rectangle(cell[\"bbox\"], outline=\"red\")\n",
    "        \n",
    "    return image, cells\n",
    "\n",
    "def get_cell_coordinates_by_row(table_data):\n",
    "    # Extract rows and columns\n",
    "    rows = [entry for entry in table_data if entry['label'] == 'table row']\n",
    "    columns = [entry for entry in table_data if entry['label'] == 'table column']\n",
    "\n",
    "    # Sort rows and columns by their Y and X coordinates, respectively\n",
    "    rows.sort(key=lambda x: x['bbox'][1])\n",
    "    columns.sort(key=lambda x: x['bbox'][0])\n",
    "\n",
    "    # Function to find cell coordinates\n",
    "    def find_cell_coordinates(row, column):\n",
    "        cell_bbox = [column['bbox'][0], row['bbox'][1], column['bbox'][2], row['bbox'][3]]\n",
    "        return cell_bbox\n",
    "\n",
    "    # Generate cell coordinates and count cells in each row\n",
    "    cell_coordinates = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_cells = []\n",
    "        for column in columns:\n",
    "            cell_bbox = find_cell_coordinates(row, column)\n",
    "            row_cells.append({'column': column['bbox'], 'cell': cell_bbox})\n",
    "\n",
    "        # Sort cells in the row by X coordinate\n",
    "        row_cells.sort(key=lambda x: x['column'][0])\n",
    "\n",
    "        # Append row information to cell_coordinates\n",
    "        cell_coordinates.append({'row': row['bbox'], 'cells': row_cells, 'cell_count': len(row_cells)})\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    cell_coordinates.sort(key=lambda x: x['row'][1])\n",
    "\n",
    "    return cell_coordinates\n",
    "\n",
    "\n",
    "def apply_ocr(cell_coordinates, cropped_table):\n",
    "    # let's OCR row by row\n",
    "    data = dict()\n",
    "    max_num_columns = 0\n",
    "    for idx, row in enumerate(cell_coordinates):\n",
    "      row_text = []\n",
    "      for cell in row[\"cells\"]:\n",
    "        # crop cell out of image\n",
    "        cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n",
    "        # apply OCR\n",
    "        result = reader.readtext(np.array(cell_image))\n",
    "        if len(result) > 0:\n",
    "          text = \" \".join([x[1] for x in result])\n",
    "          row_text.append(text)\n",
    "\n",
    "      if len(row_text) > max_num_columns:\n",
    "          max_num_columns = len(row_text)\n",
    "      \n",
    "      data[str(idx)] = row_text\n",
    "\n",
    "    # pad rows which don't have max_num_columns elements\n",
    "    # to make sure all rows have the same number of columns\n",
    "    for idx, row_data in data.copy().items():\n",
    "        if len(row_data) != max_num_columns:\n",
    "          row_data = row_data + [\"\" for _ in range(max_num_columns - len(row_data))]\n",
    "        data[str(idx)] = row_data\n",
    "\n",
    "    listo = []\n",
    "    for row, row_text in data.items():\n",
    "        listo.append(row_text)\n",
    "\n",
    "    df = pd.DataFrame(listo[1:], columns = listo[0])\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 55/8\n",
      "Page 56/8\n",
      "Page 57/8\n",
      "Page 58/8\n",
      "Page 59/8\n",
      "Page 60/8\n",
      "Page 61/8\n"
     ]
    }
   ],
   "source": [
    "def extract_tables(pathos, start, finish, out_path):\n",
    "    images = convert_from_path(pathos,dpi=200, \n",
    "                           first_page=start, last_page=finish,)\n",
    "    \n",
    "    counter = start\n",
    "    for image in images:\n",
    "\n",
    "        cropped_table = detect_and_crop_table(image)\n",
    "\n",
    "        image, cells = recognize_table(cropped_table)\n",
    "\n",
    "        cell_coordinates = get_cell_coordinates_by_row(cells)\n",
    "        \n",
    "        df = apply_ocr(cell_coordinates, image)\n",
    "\n",
    "        dumper(out_path, f\"page_{counter}\", df)\n",
    "\n",
    "        print(f\"Page {counter}/{finish}\")\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "pathos = '../input/papers/who_global_report_on_the_use_of_alcohol_taxes.pdf'\n",
    "\n",
    "extract_tables(pathos,55, 61,\n",
    "            '../inter/screenshot_test/pdf2image_test')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
